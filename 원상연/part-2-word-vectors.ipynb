{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-2-word-vectors\n### Introducing Distributed Word Vectors\n### Preparing to Train a Model"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"import pandas as pd\n\n# Read data from files \ntrain = pd.read_csv( \"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/labeledTrainData.tsv\", header=0, \n delimiter=\"\\t\", quoting=3 )\ntest = pd.read_csv( \"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/testData.tsv\", header=0, delimiter=\"\\t\", quoting=3 )\nunlabeled_train = pd.read_csv( \"../input/kumarmanoj-bag-of-words-meets-bags-of-popcorn/unlabeledTrainData.tsv\", header=0, \n delimiter=\"\\t\", quoting=3 )\n\n# Verify the number of reviews that were read (100,000 in total)\nprint (\"Read %d labeled train reviews, %d labeled test reviews, \" \\\n \"and %d unlabeled reviews\\n\" % (train[\"review\"].size,  \n test[\"review\"].size, unlabeled_train[\"review\"].size ))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import various modules for string cleaning\nfrom bs4 import BeautifulSoup\nimport re\nfrom nltk.corpus import stopwords\n\ndef review_to_wordlist( review, remove_stopwords=False ):\n    # Function to convert a document to a sequence of words,\n    # optionally removing stop words.  Returns a list of words.\n    #\n    # 1. Remove HTML\n    review_text = BeautifulSoup(review).get_text()\n    #  \n    # 2. Remove non-letters\n    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n    #\n    # 3. Convert words to lower case and split them\n    words = review_text.lower().split()\n    #\n    # 4. Optionally remove stop words (false by default)\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    #\n    # 5. Return a list of words\n    return(words)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Download the punkt tokenizer for sentence splitting\nimport nltk.data\nnltk.download()   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Load the punkt tokenizer\ntokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n\n# Define a function to split a review into parsed sentences\ndef review_to_sentences( review, tokenizer, remove_stopwords=False ):\n    # Function to split a review into parsed sentences. Returns a \n    # list of sentences, where each sentence is a list of words\n    #\n    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n    raw_sentences = tokenizer.tokenize(review.strip())\n    #\n    # 2. Loop over each sentence\n    sentences = []\n    for raw_sentence in raw_sentences:\n        # If a sentence is empty, skip it\n        if len(raw_sentence) > 0:\n            # Otherwise, call review_to_wordlist to get a list of words\n            sentences.append( review_to_wordlist( raw_sentence, \\\n              remove_stopwords ))\n    #\n    # Return the list of sentences (each sentence is a list of words,\n    # so this returns a list of lists\n    return sentences","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences = []  # Initialize an empty list of sentences\n\nprint (\"Parsing sentences from training set\")\nfor review in train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n\nprint (\"Parsing sentences from unlabeled set\")\nfor review in unlabeled_train[\"review\"]:\n    sentences += review_to_sentences(review, tokenizer)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training and Saving Your Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Import the built-in logging module and configure it so that Word2Vec \n# creates nice output messages\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n    level=logging.INFO)\n\n# Set values for various parameters\nnum_features = 300    # Word vector dimensionality                      \nmin_word_count = 40   # Minimum word count                        \nnum_workers = 4       # Number of threads to run in parallel\ncontext = 10          # Context window size                                                                                    \ndownsampling = 1e-3   # Downsample setting for frequent words\n\n# Initialize and train the model (this will take some time)\nfrom gensim.models import word2vec\nprint (\"Training model...\")\nmodel = word2vec.Word2Vec(sentences, workers=num_workers, \\\n            size=num_features, min_count = min_word_count, \\\n            window = context, sample = downsampling)\n\n# If you don't plan to train the model any further, calling \n# init_sims will make the model much more memory-efficient.\nmodel.init_sims(replace=True)\n\n# It can be helpful to create a meaningful model name and \n# save the model for later use. You can load it later using Word2Vec.load()\nmodel_name = \"300features_40minwords_10context\"\nmodel.save(model_name)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploring the Model Results"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.doesnt_match(\"man woman child kitchen\".split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.doesnt_match(\"france england germany berlin\".split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.doesnt_match(\"paris berlin london austria\".split())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar(\"man\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar(\"queen\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.most_similar(\"awful\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"https://www.kaggle.com/c/word2vec-nlp-tutorial/overview/part-3-more-fun-with-word-vectors\n## part-3-more-fun-with-word-vectors"},{"metadata":{"trusted":true},"cell_type":"code","source":"from gensim.models import Word2Vec\nmodel = Word2Vec.load(\"./300features_40minwords_10context\")\nmodel[\"flower\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"type(model.syn1neg)","execution_count":61,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn1neg` (Attribute will be removed in 4.0.0, use self.trainables.syn1neg instead).\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"},{"output_type":"execute_result","execution_count":61,"data":{"text/plain":"numpy.ndarray"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.syn1neg.shape","execution_count":60,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn1neg` (Attribute will be removed in 4.0.0, use self.trainables.syn1neg instead).\n  \"\"\"Entry point for launching an IPython kernel.\n","name":"stderr"},{"output_type":"execute_result","execution_count":60,"data":{"text/plain":"(16490, 300)"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"### From Words To Paragraphs, Attempt 1: Vector Averaging"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.","execution_count":62,"outputs":[{"output_type":"execute_result","execution_count":62,"data":{"text/plain":"<bound method Word2Vec.save_word2vec_format of <gensim.models.word2vec.Word2Vec object at 0x7f1dd1868490>>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np  # Make sure that numpy is imported\n\ndef makeFeatureVec(words, model, num_features):\n    # Function to average all of the word vectors in a given\n    # paragraph\n    #\n    # Pre-initialize an empty numpy array (for speed)\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    #\n    nwords = 0.\n    # \n    # Index2word is a list that contains the names of the words in \n    # the model's vocabulary. Convert it to a set, for speed \n    index2word_set = set(model.index2word)\n    #\n    # Loop over each word in the review and, if it is in the model's\n    # vocaublary, add its feature vector to the total\n    for word in words:\n        if word in index2word_set: \n            nwords = nwords + 1.\n            featureVec = np.add(featureVec,model[word])\n    # \n    # Divide the result by the number of words to get the average\n    featureVec = np.divide(featureVec,nwords)\n    return featureVec\n\n\ndef getAvgFeatureVecs(reviews, model, num_features):\n    # Given a set of reviews (each one a list of words), calculate \n    # the average feature vector for each one and return a 2D numpy array \n    # \n    # Initialize a counter\n    counter = 0.\n    # \n    # Preallocate a 2D numpy array, for speed\n    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n    # \n    # Loop through the reviews\n    for review in reviews:\n       #\n       # Print a status message every 1000th review\n       if counter%1000. == 0.:\n           print (\"Review %d of %d\" % (counter, len(reviews)))\n       # \n       # Call the function (defined above) that makes average feature vectors\n       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n           num_features)\n       #\n       # Increment the counter\n       counter = counter + 1.\n    return reviewFeatureVecs","execution_count":63,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-output":true},"cell_type":"code","source":"# Calculate average feature vectors for training and testing sets,\n# using the functions we defined above. Notice that we now use stop word\n# removal.\n\nclean_train_reviews = []\nfor review in train[\"review\"]:\n    clean_train_reviews.append( review_to_wordlist( review, \\\n        remove_stopwords=True ))\n\ntrainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n\nprint (\"Creating average feature vecs for test reviews\")\nclean_test_reviews = []\nfor review in test[\"review\"]:\n    clean_test_reviews.append( review_to_wordlist( review, \\\n        remove_stopwords=True ))\n\ntestDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )","execution_count":64,"outputs":[{"output_type":"stream","text":"Review 0 of 25000\n","name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"'method' object is not iterable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-64-f2fa72a06679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         remove_stopwords=True ))\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrainDataVecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetAvgFeatureVecs\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mclean_train_reviews\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating average feature vecs for test reviews\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-63-9c3a4cad3712>\u001b[0m in \u001b[0;36mgetAvgFeatureVecs\u001b[0;34m(reviews, model, num_features)\u001b[0m\n\u001b[1;32m     45\u001b[0m        \u001b[0;31m# Call the function (defined above) that makes average feature vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m        reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n\u001b[0;32m---> 47\u001b[0;31m            num_features)\n\u001b[0m\u001b[1;32m     48\u001b[0m        \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m        \u001b[0;31m# Increment the counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-63-9c3a4cad3712>\u001b[0m in \u001b[0;36mmakeFeatureVec\u001b[0;34m(words, model, num_features)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Index2word is a list that contains the names of the words in\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# the model's vocabulary. Convert it to a set, for speed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mindex2word_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_word2vec_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# Loop over each word in the review and, if it is in the model's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"]}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}